# 寄存点项目优化方案详细设计

## 项目架构概览

基于你的项目模块，我们采用微服务架构，主要包含以下服务：

```
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   用户端服务     │    │   管理端服务     │    │   硬件控制服务   │
│                │    │                │    │                │
│ - 城市选择      │    │ - 柜组管理      │    │ - 柜口控制      │
│ - 我的附近      │    │ - 柜口管理      │    │ - 状态同步      │
│ - 搜索寄存点    │    │ - 网点地图      │    │ - 故障检测      │
│ - 城市分布图    │    │                │    │                │
└─────────────────┘    └─────────────────┘    └─────────────────┘
         │                       │                       │
         └───────────────────────┼───────────────────────┘
                                 │
                    ┌─────────────────┐
                    │   数据服务层     │
                    │                │
                    │ - Redis缓存     │
                    │ - MySQL主从     │
                    │ - 消息队列      │
                    └─────────────────┘
```

## 核心优化方案

### 1. 地理位置查询优化

#### 1.1 空间索引优化
```sql
-- 创建空间索引
CREATE SPATIAL INDEX idx_location ON storage_points(location);

-- 优化的地理查询
SELECT 
    id, name, address, longitude, latitude,
    ST_Distance_Sphere(location, POINT(?, ?)) as distance
FROM storage_points 
WHERE ST_Distance_Sphere(location, POINT(?, ?)) <= ?
    AND status = 1
    AND city_id = ?
ORDER BY distance
LIMIT 20;
```

#### 1.2 地理位置缓存策略
```go
// 地理位置缓存管理器
type LocationCacheManager struct {
    redis    *redis.Client
    localCache *sync.Map
    geoHashPrecision int
}

func (lcm *LocationCacheManager) GetNearbyPoints(lat, lng float64, radius int) ([]*StoragePoint, error) {
    // 使用GeoHash作为缓存key
    geoHash := geohash.Encode(lat, lng, lcm.geoHashPrecision)
    cacheKey := fmt.Sprintf("nearby:%s:%d", geoHash, radius)
    
    // 本地缓存查询
    if cached, ok := lcm.localCache.Load(cacheKey); ok {
        return cached.([]*StoragePoint), nil
    }
    
    // Redis缓存查询
    if data := lcm.redis.Get(cacheKey).Val(); data != "" {
        points := lcm.deserialize(data)
        lcm.localCache.Store(cacheKey, points)
        return points, nil
    }
    
    // 数据库查询
    points, err := lcm.queryFromDB(lat, lng, radius)
    if err != nil {
        return nil, err
    }
    
    // 写入缓存
    lcm.redis.Set(cacheKey, lcm.serialize(points), 10*time.Minute)
    lcm.localCache.Store(cacheKey, points)
    
    return points, nil
}
```

### 2. 城市数据管理优化

#### 2.1 城市数据预加载
```go
// 城市数据管理器
type CityDataManager struct {
    cache map[int64]*CityData
    mutex sync.RWMutex
    updateChan chan CityUpdateEvent
}

type CityData struct {
    ID           int64
    Name         string
    StoragePoints []*StoragePoint
    Boundary     [][]float64
    LastUpdate   time.Time
}

// 预加载热门城市数据
func (cdm *CityDataManager) PreloadHotCities() {
    hotCities := []int64{1, 2, 3, 4, 5} // 北上广深杭
    
    for _, cityID := range hotCities {
        go func(id int64) {
            cityData, err := cdm.loadCityData(id)
            if err != nil {
                log.Errorf("Failed to preload city %d: %v", id, err)
                return
            }
            
            cdm.mutex.Lock()
            cdm.cache[id] = cityData
            cdm.mutex.Unlock()
            
            log.Infof("Preloaded city %d with %d storage points", 
                id, len(cityData.StoragePoints))
        }(cityID)
    }
}

// 增量更新城市数据
func (cdm *CityDataManager) HandleCityUpdate(event CityUpdateEvent) {
    cdm.mutex.Lock()
    defer cdm.mutex.Unlock()
    
    cityData, exists := cdm.cache[event.CityID]
    if !exists {
        return
    }
    
    switch event.Type {
    case "storage_point_added":
        cityData.StoragePoints = append(cityData.StoragePoints, event.StoragePoint)
    case "storage_point_updated":
        cdm.updateStoragePoint(cityData, event.StoragePoint)
    case "storage_point_removed":
        cdm.removeStoragePoint(cityData, event.StoragePointID)
    }
    
    cityData.LastUpdate = time.Now()
}
```

### 3. 柜组和柜口管理优化

#### 3.1 柜口状态管理
```go
// 柜口状态管理器
type CabinetDoorManager struct {
    doors       map[int64]*CabinetDoor
    statusLock  sync.RWMutex
    eventBus    *EventBus
    hardware    HardwareController
    db          *sql.DB
}

type CabinetDoor struct {
    ID           int64
    GroupID      int64
    DoorNumber   string
    Status       DoorStatus
    CurrentOrder int64
    LastOpened   time.Time
    mutex        sync.RWMutex
}

// 原子性状态更新
func (cdm *CabinetDoorManager) UpdateDoorStatus(doorID int64, newStatus DoorStatus, orderID int64) error {
    cdm.statusLock.Lock()
    defer cdm.statusLock.Unlock()
    
    door, exists := cdm.doors[doorID]
    if !exists {
        return errors.New("door not found")
    }
    
    door.mutex.Lock()
    defer door.mutex.Unlock()
    
    // 状态转换验证
    if !cdm.isValidStatusTransition(door.Status, newStatus) {
        return errors.New("invalid status transition")
    }
    
    // 数据库事务更新
    tx, err := cdm.db.Begin()
    if err != nil {
        return err
    }
    defer tx.Rollback()
    
    // 更新柜口状态
    _, err = tx.Exec(`
        UPDATE cabinet_doors 
        SET status = ?, current_order_id = ?, updated_at = NOW() 
        WHERE id = ?
    `, newStatus, orderID, doorID)
    if err != nil {
        return err
    }
    
    // 记录操作日志
    _, err = tx.Exec(`
        INSERT INTO cabinet_door_logs 
        (door_id, operation_type, order_id, operation_time, result) 
        VALUES (?, ?, ?, NOW(), 'success')
    `, doorID, "status_change", orderID)
    if err != nil {
        return err
    }
    
    if err = tx.Commit(); err != nil {
        return err
    }
    
    // 更新内存状态
    door.Status = newStatus
    door.CurrentOrder = orderID
    door.LastOpened = time.Now()
    
    // 发送状态变更事件
    cdm.eventBus.Publish(DoorStatusChangedEvent{
        DoorID:    doorID,
        OldStatus: door.Status,
        NewStatus: newStatus,
        OrderID:   orderID,
        Timestamp: time.Now(),
    })
    
    return nil
}
```

#### 3.2 柜口操作队列
```go
// 柜口操作队列管理
type DoorOperationQueue struct {
    queues map[int64]chan DoorOperation
    mutex  sync.RWMutex
    workers map[int64]*OperationWorker
}

type DoorOperation struct {
    ID          string
    DoorID      int64
    Operation   OperationType
    OrderID     int64
    UserID      int64
    Priority    int
    Timeout     time.Duration
    ResultChan  chan OperationResult
    CreatedAt   time.Time
}

type OperationWorker struct {
    doorID   int64
    queue    chan DoorOperation
    hardware HardwareController
    running  bool
    mutex    sync.Mutex
}

func (doq *DoorOperationQueue) SubmitOperation(op DoorOperation) <-chan OperationResult {
    doq.mutex.RLock()
    worker, exists := doq.workers[op.DoorID]
    doq.mutex.RUnlock()
    
    if !exists {
        doq.mutex.Lock()
        worker = &OperationWorker{
            doorID:   op.DoorID,
            queue:    make(chan DoorOperation, 100),
            hardware: doq.hardware,
        }
        doq.workers[op.DoorID] = worker
        go worker.Start()
        doq.mutex.Unlock()
    }
    
    op.ResultChan = make(chan OperationResult, 1)
    op.ID = generateOperationID()
    op.CreatedAt = time.Now()
    
    select {
    case worker.queue <- op:
        return op.ResultChan
    case <-time.After(5 * time.Second):
        result := OperationResult{
            OperationID: op.ID,
            Success:     false,
            Error:       "operation queue timeout",
            Duration:    time.Since(op.CreatedAt),
        }
        op.ResultChan <- result
        return op.ResultChan
    }
}

func (ow *OperationWorker) Start() {
    ow.mutex.Lock()
    if ow.running {
        ow.mutex.Unlock()
        return
    }
    ow.running = true
    ow.mutex.Unlock()
    
    for op := range ow.queue {
        result := ow.executeOperation(op)
        op.ResultChan <- result
    }
}

func (ow *OperationWorker) executeOperation(op DoorOperation) OperationResult {
    startTime := time.Now()
    
    // 设置操作超时
    ctx, cancel := context.WithTimeout(context.Background(), op.Timeout)
    defer cancel()
    
    var err error
    switch op.Operation {
    case OperationTypeOpen:
        err = ow.hardware.OpenDoorWithContext(ctx, op.DoorID)
    case OperationTypeClose:
        err = ow.hardware.CloseDoorWithContext(ctx, op.DoorID)
    case OperationTypeCheck:
        err = ow.hardware.CheckDoorStatusWithContext(ctx, op.DoorID)
    }
    
    return OperationResult{
        OperationID: op.ID,
        Success:     err == nil,
        Error:       errorToString(err),
        Duration:    time.Since(startTime),
    }
}
```

### 4. 硬件通信优化

#### 4.1 连接池管理
```go
// 硬件连接池
type HardwareConnectionPool struct {
    connections chan *HardwareConnection
    factory     ConnectionFactory
    maxSize     int
    currentSize int32
    mutex       sync.Mutex
}

type HardwareConnection struct {
    ID         string
    Connection net.Conn
    LastUsed   time.Time
    InUse      bool
    CreatedAt  time.Time
}

func NewHardwareConnectionPool(factory ConnectionFactory, maxSize int) *HardwareConnectionPool {
    return &HardwareConnectionPool{
        connections: make(chan *HardwareConnection, maxSize),
        factory:     factory,
        maxSize:     maxSize,
    }
}

func (hcp *HardwareConnectionPool) GetConnection() (*HardwareConnection, error) {
    select {
    case conn := <-hcp.connections:
        // 检查连接是否有效
        if time.Since(conn.LastUsed) > 5*time.Minute {
            conn.Connection.Close()
            return hcp.createNewConnection()
        }
        conn.InUse = true
        conn.LastUsed = time.Now()
        return conn, nil
    default:
        // 池中没有可用连接，创建新连接
        if atomic.LoadInt32(&hcp.currentSize) < int32(hcp.maxSize) {
            return hcp.createNewConnection()
        }
        // 等待连接归还
        select {
        case conn := <-hcp.connections:
            conn.InUse = true
            conn.LastUsed = time.Now()
            return conn, nil
        case <-time.After(10 * time.Second):
            return nil, errors.New("connection pool timeout")
        }
    }
}

func (hcp *HardwareConnectionPool) ReturnConnection(conn *HardwareConnection) {
    conn.InUse = false
    conn.LastUsed = time.Now()
    
    select {
    case hcp.connections <- conn:
        // 成功归还到池中
    default:
        // 池已满，关闭连接
        conn.Connection.Close()
        atomic.AddInt32(&hcp.currentSize, -1)
    }
}

func (hcp *HardwareConnectionPool) createNewConnection() (*HardwareConnection, error) {
    conn, err := hcp.factory.CreateConnection()
    if err != nil {
        return nil, err
    }
    
    atomic.AddInt32(&hcp.currentSize, 1)
    
    return &HardwareConnection{
        ID:         generateConnectionID(),
        Connection: conn,
        LastUsed:   time.Now(),
        InUse:      true,
        CreatedAt:  time.Now(),
    }, nil
}
```

#### 4.2 硬件通信重试机制
```go
// 硬件控制器
type HardwareController struct {
    connectionPool *HardwareConnectionPool
    retryPolicy    RetryPolicy
    circuitBreaker *CircuitBreaker
    metrics        *HardwareMetrics
}

type RetryPolicy struct {
    MaxRetries int
    BaseDelay  time.Duration
    MaxDelay   time.Duration
    Multiplier float64
}

func (hc *HardwareController) OpenDoorWithContext(ctx context.Context, doorID int64) error {
    return hc.executeWithRetry(ctx, func() error {
        return hc.openDoor(doorID)
    })
}

func (hc *HardwareController) executeWithRetry(ctx context.Context, operation func() error) error {
    var lastErr error
    delay := hc.retryPolicy.BaseDelay
    
    for i := 0; i <= hc.retryPolicy.MaxRetries; i++ {
        // 检查熔断器状态
        if !hc.circuitBreaker.AllowRequest() {
            return errors.New("circuit breaker is open")
        }
        
        // 执行操作
        err := operation()
        if err == nil {
            hc.circuitBreaker.RecordSuccess()
            hc.metrics.RecordSuccess()
            return nil
        }
        
        lastErr = err
        hc.circuitBreaker.RecordFailure()
        hc.metrics.RecordFailure()
        
        // 最后一次重试失败
        if i == hc.retryPolicy.MaxRetries {
            break
        }
        
        // 等待重试
        select {
        case <-ctx.Done():
            return ctx.Err()
        case <-time.After(delay):
            delay = time.Duration(float64(delay) * hc.retryPolicy.Multiplier)
            if delay > hc.retryPolicy.MaxDelay {
                delay = hc.retryPolicy.MaxDelay
            }
        }
    }
    
    return fmt.Errorf("operation failed after %d retries: %v", 
        hc.retryPolicy.MaxRetries, lastErr)
}

func (hc *HardwareController) openDoor(doorID int64) error {
    conn, err := hc.connectionPool.GetConnection()
    if err != nil {
        return err
    }
    defer hc.connectionPool.ReturnConnection(conn)
    
    // 发送开门命令
    command := HardwareCommand{
        Type:   "OPEN_DOOR",
        DoorID: doorID,
        Timestamp: time.Now(),
    }
    
    data, err := json.Marshal(command)
    if err != nil {
        return err
    }
    
    // 设置写入超时
    conn.Connection.SetWriteDeadline(time.Now().Add(5 * time.Second))
    _, err = conn.Connection.Write(data)
    if err != nil {
        return err
    }
    
    // 读取响应
    conn.Connection.SetReadDeadline(time.Now().Add(10 * time.Second))
    response := make([]byte, 1024)
    n, err := conn.Connection.Read(response)
    if err != nil {
        return err
    }
    
    var result HardwareResponse
    err = json.Unmarshal(response[:n], &result)
    if err != nil {
        return err
    }
    
    if !result.Success {
        return errors.New(result.ErrorMessage)
    }
    
    return nil
}
```

### 5. 实时数据同步优化

#### 5.1 WebSocket连接管理
```go
// WebSocket连接管理器
type WebSocketManager struct {
    connections map[string]*WebSocketConnection
    mutex       sync.RWMutex
    eventBus    *EventBus
    upgrader    websocket.Upgrader
}

type WebSocketConnection struct {
    ID       string
    UserID   int64
    UserType string // "user" or "admin"
    Conn     *websocket.Conn
    Send     chan []byte
    LastPing time.Time
    Filters  map[string]interface{} // 订阅过滤器
}

func (wsm *WebSocketManager) HandleConnection(w http.ResponseWriter, r *http.Request) {
    conn, err := wsm.upgrader.Upgrade(w, r, nil)
    if err != nil {
        log.Errorf("WebSocket upgrade failed: %v", err)
        return
    }
    
    wsConn := &WebSocketConnection{
        ID:       generateConnectionID(),
        Conn:     conn,
        Send:     make(chan []byte, 256),
        LastPing: time.Now(),
        Filters:  make(map[string]interface{}),
    }
    
    // 从请求中获取用户信息
    wsConn.UserID = getUserIDFromRequest(r)
    wsConn.UserType = getUserTypeFromRequest(r)
    
    wsm.mutex.Lock()
    wsm.connections[wsConn.ID] = wsConn
    wsm.mutex.Unlock()
    
    // 启动读写协程
    go wsConn.writePump()
    go wsConn.readPump(wsm)
}

func (wsc *WebSocketConnection) writePump() {
    ticker := time.NewTicker(54 * time.Second)
    defer func() {
        ticker.Stop()
        wsc.Conn.Close()
    }()
    
    for {
        select {
        case message, ok := <-wsc.Send:
            wsc.Conn.SetWriteDeadline(time.Now().Add(10 * time.Second))
            if !ok {
                wsc.Conn.WriteMessage(websocket.CloseMessage, []byte{})
                return
            }
            
            if err := wsc.Conn.WriteMessage(websocket.TextMessage, message); err != nil {
                log.Errorf("WebSocket write error: %v", err)
                return
            }
            
        case <-ticker.C:
            wsc.Conn.SetWriteDeadline(time.Now().Add(10 * time.Second))
            if err := wsc.Conn.WriteMessage(websocket.PingMessage, nil); err != nil {
                return
            }
        }
    }
}

// 广播状态更新
func (wsm *WebSocketManager) BroadcastStatusUpdate(event StatusUpdateEvent) {
    message, err := json.Marshal(event)
    if err != nil {
        log.Errorf("Failed to marshal status update: %v", err)
        return
    }
    
    wsm.mutex.RLock()
    defer wsm.mutex.RUnlock()
    
    for _, conn := range wsm.connections {
        // 根据用户类型和过滤器决定是否发送
        if wsm.shouldSendToConnection(conn, event) {
            select {
            case conn.Send <- message:
            default:
                // 发送缓冲区满，关闭连接
                close(conn.Send)
                delete(wsm.connections, conn.ID)
            }
        }
    }
}
```

#### 5.2 消息队列处理
```go
// 消息队列处理器
type MessageQueueProcessor struct {
    producer kafka.Producer
    consumer kafka.Consumer
    handlers map[string]MessageHandler
    workers  int
}

type MessageHandler interface {
    Handle(message *kafka.Message) error
}

// 状态更新处理器
type StatusUpdateHandler struct {
    doorManager *CabinetDoorManager
    wsManager   *WebSocketManager
    cache       *redis.Client
}

func (suh *StatusUpdateHandler) Handle(message *kafka.Message) error {
    var event StatusUpdateEvent
    err := json.Unmarshal(message.Value, &event)
    if err != nil {
        return err
    }
    
    // 更新柜口状态
    err = suh.doorManager.UpdateDoorStatus(event.DoorID, event.NewStatus, event.OrderID)
    if err != nil {
        log.Errorf("Failed to update door status: %v", err)
        return err
    }
    
    // 更新缓存
    cacheKey := fmt.Sprintf("door_status:%d", event.DoorID)
    statusData, _ := json.Marshal(map[string]interface{}{
        "status":     event.NewStatus,
        "order_id":   event.OrderID,
        "updated_at": time.Now(),
    })
    suh.cache.Set(cacheKey, statusData, 1*time.Hour)
    
    // 广播给WebSocket客户端
    suh.wsManager.BroadcastStatusUpdate(event)
    
    return nil
}

// 批量处理消息
func (mqp *MessageQueueProcessor) StartBatchProcessor() {
    for i := 0; i < mqp.workers; i++ {
        go func(workerID int) {
            batch := make([]*kafka.Message, 0, 100)
            ticker := time.NewTicker(1 * time.Second)
            defer ticker.Stop()
            
            for {
                select {
                case message := <-mqp.consumer.Messages():
                    batch = append(batch, message)
                    
                    // 批量大小达到阈值或超时，处理批量消息
                    if len(batch) >= 100 {
                        mqp.processBatch(batch)
                        batch = batch[:0]
                    }
                    
                case <-ticker.C:
                    if len(batch) > 0 {
                        mqp.processBatch(batch)
                        batch = batch[:0]
                    }
                }
            }
        }(i)
    }
}
```

### 6. 监控和告警系统

#### 6.1 性能监控
```go
// 性能监控器
type PerformanceMonitor struct {
    metrics map[string]*Metric
    mutex   sync.RWMutex
    reporter MetricReporter
}

type Metric struct {
    Name      string
    Type      MetricType
    Value     float64
    Labels    map[string]string
    Timestamp time.Time
}

func (pm *PerformanceMonitor) RecordAPILatency(endpoint string, duration time.Duration) {
    pm.mutex.Lock()
    defer pm.mutex.Unlock()
    
    metricName := fmt.Sprintf("api_latency_%s", endpoint)
    pm.metrics[metricName] = &Metric{
        Name:      metricName,
        Type:      MetricTypeHistogram,
        Value:     float64(duration.Milliseconds()),
        Labels:    map[string]string{"endpoint": endpoint},
        Timestamp: time.Now(),
    }
}

func (pm *PerformanceMonitor) RecordDoorOperation(doorID int64, operation string, success bool, duration time.Duration) {
    pm.mutex.Lock()
    defer pm.mutex.Unlock()
    
    labels := map[string]string{
        "door_id":   fmt.Sprintf("%d", doorID),
        "operation": operation,
        "success":   fmt.Sprintf("%t", success),
    }
    
    // 记录操作耗时
    pm.metrics["door_operation_duration"] = &Metric{
        Name:      "door_operation_duration",
        Type:      MetricTypeHistogram,
        Value:     float64(duration.Milliseconds()),
        Labels:    labels,
        Timestamp: time.Now(),
    }
    
    // 记录操作计数
    pm.metrics["door_operation_total"] = &Metric{
        Name:      "door_operation_total",
        Type:      MetricTypeCounter,
        Value:     1,
        Labels:    labels,
        Timestamp: time.Now(),
    }
}

// 定期上报指标
func (pm *PerformanceMonitor) StartReporting() {
    ticker := time.NewTicker(30 * time.Second)
    defer ticker.Stop()
    
    for range ticker.C {
        pm.mutex.RLock()
        metrics := make([]*Metric, 0, len(pm.metrics))
        for _, metric := range pm.metrics {
            metrics = append(metrics, metric)
        }
        pm.mutex.RUnlock()
        
        if len(metrics) > 0 {
            pm.reporter.Report(metrics)
        }
    }
}
```

#### 6.2 告警系统
```go
// 告警管理器
type AlertManager struct {
    rules     []AlertRule
    channels  map[string]AlertChannel
    history   *AlertHistory
    mutex     sync.RWMutex
}

type AlertRule struct {
    Name        string
    Condition   string
    Threshold   float64
    Duration    time.Duration
    Severity    AlertSeverity
    Channels    []string
    Enabled     bool
}

type AlertChannel interface {
    Send(alert Alert) error
}

func (am *AlertManager) CheckAlerts(metrics []*Metric) {
    for _, rule := range am.rules {
        if !rule.Enabled {
            continue
        }
        
        triggered := am.evaluateRule(rule, metrics)
        if triggered {
            alert := Alert{
                RuleName:  rule.Name,
                Severity:  rule.Severity,
                Message:   am.generateAlertMessage(rule, metrics),
                Timestamp: time.Now(),
            }
            
            am.sendAlert(alert, rule.Channels)
        }
    }
}

func (am *AlertManager) sendAlert(alert Alert, channelNames []string) {
    for _, channelName := range channelNames {
        if channel, exists := am.channels[channelName]; exists {
            go func(ch AlertChannel, a Alert) {
                if err := ch.Send(a); err != nil {
                    log.Errorf("Failed to send alert via %s: %v", channelName, err)
                }
            }(channel, alert)
        }
    }
    
    // 记录告警历史
    am.history.Record(alert)
}

// 钉钉告警通道
type DingTalkAlertChannel struct {
    WebhookURL string
    Secret     string
}

func (dt *DingTalkAlertChannel) Send(alert Alert) error {
    message := map[string]interface{}{
        "msgtype": "text",
        "text": map[string]string{
            "content": fmt.Sprintf("【%s】%s\n时间：%s", 
                alert.Severity, alert.Message, alert.Timestamp.Format("2006-01-02 15:04:05")),
        },
    }
    
    data, err := json.Marshal(message)
    if err != nil {
        return err
    }
    
    resp, err := http.Post(dt.WebhookURL, "application/json", bytes.NewBuffer(data))
    if err != nil {
        return err
    }
    defer resp.Body.Close()
    
    return nil
}
```

## 部署和运维优化

### 1. 容器化部署
```dockerfile
# 多阶段构建
FROM golang:1.19-alpine AS builder

WORKDIR /app
COPY go.mod go.sum ./
RUN go mod download

COPY . .
RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o main .

FROM alpine:latest
RUN apk --no-cache add ca-certificates tzdata
WORKDIR /root/

COPY --from=builder /app/main .
COPY --from=builder /app/configs ./configs

EXPOSE 8080
CMD ["./main"]
```

### 2. Kubernetes部署配置
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: storage-service
spec:
  replicas: 3
  selector:
    matchLabels:
      app: storage-service
  template:
    metadata:
      labels:
        app: storage-service
    spec:
      containers:
      - name: storage-service
        image: storage-service:latest
        ports:
        - containerPort: 8080
        env:
        - name: DB_HOST
          valueFrom:
            secretKeyRef:
              name: db-secret
              key: host
        - name: REDIS_HOST
          valueFrom:
            configMapKeyRef:
              name: redis-config
              key: host
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  name: storage-service
spec:
  selector:
    app: storage-service
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8080
  type: LoadBalancer
```

### 3. 监控配置
```yaml
# Prometheus配置
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'storage-service'
    static_configs:
      - targets: ['storage-service:8080']
    metrics_path: /metrics
    scrape_interval: 10s

rule_files:
  - "alert_rules.yml"

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093
```

## 性能基准和优化目标

### 1. 性能指标
```yaml
performance_targets:
  api_response_time:
    nearby_search: 
      p95: 200ms
      p99: 500ms
    city_switch:
      p95: 100ms
      p99: 300ms
    door_operation:
      p95: 3s
      p99: 5s
  
  throughput:
    concurrent_users: 10000
    requests_per_second: 5000
    door_operations_per_minute: 1000
  
  availability:
    system_uptime: 99.9%
    door_availability: 99.5%
  
  cache_performance:
    hit_rate: 90%
    response_time: 10ms
```

### 2. 容量规划
```yaml
capacity_planning:
  database:
    storage_points: 100000
    cabinet_doors: 1000000
    daily_operations: 500000
    data_retention: 2_years
  
  cache:
    redis_memory: 16GB
    local_cache: 2GB_per_instance
  
  hardware:
    cpu_cores: 8_per_instance
    memory: 16GB_per_instance
    network: 1Gbps
```

## 总结

这份优化方案针对你的项目模块提供了全面的技术解决方案：

### 核心优化点：
1. **地理位置查询**: 空间索引 + 多级缓存
2. **城市数据管理**: 预加载 + 增量更新
3. **柜口状态管理**: 原子操作 + 事件驱动
4. **硬件通信**: 连接池 + 重试机制 + 熔断器
5. **实时同步**: WebSocket + 消息队列
6. **监控告警**: 全方位监控 + 智能告警

### 实施建议：
1. **第一阶段**: 基础优化（缓存、索引、连接池）
2. **第二阶段**: 高级功能（实时同步、监控告警）
3. **第三阶段**: 智能化（故障预测、自动运维）

这个方案可以支撑大规模并发访问，保证系统的高可用性和高性能。